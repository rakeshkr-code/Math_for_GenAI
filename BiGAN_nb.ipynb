{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device : {device}\")\n",
    "\n",
    "# Configurations\n",
    "batch_size = 128\n",
    "latent_dim = 50\n",
    "epochs = 30\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (MNIST)\n",
    "# ----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "# ----------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687dc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator (Joint: x, z)\n",
    "# ----------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784 + latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)  # No sigmoid! BCEWithLogitsLoss handles that\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        xz = torch.cat([x, z], dim=1)\n",
    "        return self.net(xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "# ----------------------------\n",
    "G = Generator().to(device)\n",
    "E = Encoder().to(device)\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers and Loss\n",
    "# ----------------------------\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_GE = optim.Adam(list(G.parameters()) + list(E.parameters()), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a80860",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "p = 1\n",
    "os.makedirs(\"bigan\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (x_real, _) in enumerate(train_loader):\n",
    "        x_real = x_real.to(device)\n",
    "        batch_size = x_real.size(0)\n",
    "\n",
    "        # Sample z from prior\n",
    "        z_real = torch.randn(batch_size, latent_dim, device=device)\n",
    "\n",
    "        # ----------------------\n",
    "        # 1. Train Discriminator\n",
    "        # ----------------------\n",
    "        for _ in range(p):\n",
    "          G.eval()\n",
    "          E.eval()\n",
    "          D.train()\n",
    "\n",
    "          x_fake = G(z_real).detach()\n",
    "          z_fake = E(x_real).detach()\n",
    "\n",
    "          D_real = D(x_real, z_fake)\n",
    "          D_fake = D(x_fake, z_real)\n",
    "\n",
    "          label_real = torch.ones_like(D_real)\n",
    "          label_fake = torch.zeros_like(D_fake)\n",
    "\n",
    "          loss_D = bce_loss(D_real, label_real) + bce_loss(D_fake, label_fake)\n",
    "\n",
    "          optimizer_D.zero_grad()\n",
    "          loss_D.backward()\n",
    "          optimizer_D.step()\n",
    "\n",
    "        # --------------------------\n",
    "        # 2. Train Generator + Encoder\n",
    "        # --------------------------\n",
    "        for _ in range(k):\n",
    "          G.train()\n",
    "          E.train()\n",
    "          D.eval()\n",
    "\n",
    "          x_fake = G(z_real)\n",
    "          z_fake = E(x_real)\n",
    "\n",
    "          D_real = D(x_real, z_fake)\n",
    "          D_fake = D(x_fake, z_real)\n",
    "\n",
    "          loss_GE = bce_loss(D_real, label_fake) + bce_loss(D_fake, label_real)\n",
    "\n",
    "          optimizer_GE.zero_grad()\n",
    "          loss_GE.backward()\n",
    "          optimizer_GE.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(train_loader)}\",\n",
    "                  f\"Loss D: {loss_D.item():.4f}, Loss GE: {loss_GE.item():.4f}\")\n",
    "\n",
    "        # Save samples every epoch\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(64, latent_dim, device=device)\n",
    "            samples = G(z)\n",
    "            samples = samples * 0.5 + 0.5  # Denormalize\n",
    "            save_image(samples, f\"bigan/epoch_{epoch}.png\", nrow=8)\n",
    "        G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Samples for Visualization\n",
    "# ----------------------------\n",
    "def show_samples():\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(64, latent_dim, device=device)\n",
    "        x_gen = G(z).view(-1, 1, 28, 28).cpu()\n",
    "        grid = torchvision.utils.make_grid(x_gen, nrow=8)\n",
    "        plt.imshow(grid.permute(1, 2, 0).squeeze())\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Generated Samples from BiGAN\")\n",
    "        plt.show()\n",
    "\n",
    "show_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c203a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p8kjVKoDthhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "# import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "z_dim = 100\n",
        "image_size = 28\n",
        "num_classes = 10\n",
        "channels = 1\n",
        "epochs = 50\n",
        "lr = 0.0002\n",
        "beta1 = 0.5 # Adam optimizer beta1\n",
        "# beta2 = 0.999\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide : {device}\")\n",
        "\n",
        "# Create output folder\n",
        "# if not os.path.exists(\"generated_imgs\"):\n",
        "os.makedirs(\"c_gan_generated\", exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAXnM5ybXS5M",
        "outputId": "1710d685-bf3e-4579-ab47-7950e2a015c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform: Normalize images between [-1, 1] (because Tanh will be used as output)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))    # Normalize between [-1, 1]\n",
        "])\n",
        "\n",
        "# Load MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "oNsOrVYewFgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21ed48b-8e93-402f-8ebb-5d542aab3793"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.54MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.23MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceda4a32"
      },
      "source": [
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, num_classes, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.img_shape = img_shape\n",
        "        input_dim = z_dim + num_classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (N, z_dim, 1, 1)\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, noise, labels):\n",
        "        # Concatenate noise and label embedding\n",
        "        x = torch.cat([noise, self.label_emb(labels)], dim=1)\n",
        "        img = self.model(x)\n",
        "        img = img.view(x.size(0), *self.img_shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "xcAAtxsgzZ0g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        input_dim = int(torch.prod(torch.tensor(img_shape))) + num_classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (N, 1, 28, 28)\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, img, labels):\n",
        "        # Flatten image and concatenate label\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        x = torch.cat([img_flat, self.label_emb(labels)], dim=1)\n",
        "        validity = self.model(x)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "rN6u3tlq6Y23"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = (channels, image_size, image_size)\n",
        "# Models\n",
        "generator = Generator(z_dim, num_classes, img_shape).to(device)\n",
        "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Loss - Binary Cross Entropy Loss\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "kxCRtq_HCxeT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(epoch):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, z_dim, 1, 1).to(device)\n",
        "        fake_images = generator(z)\n",
        "        fake_images = fake_images * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "        save_image(fake_images, f\"generated_imgs/sample_epoch_{epoch}.png\", nrow=8)\n",
        "    generator.train()"
      ],
      "metadata": {
        "id": "fUwFbxPs9o79"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3   # Generator updates per iteration\n",
        "p = 1   # Discriminator updates per iterations"
      ],
      "metadata": {
        "id": "FCtxK7Bui44H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, epochs+1):\n",
        "    for i, (real_imgs, real_labels) in enumerate(train_loader):\n",
        "        batch_size_curr = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        real_labels = real_labels.to(device)\n",
        "\n",
        "        # Use the actual batch size for creating target tensors\n",
        "        real = torch.ones(batch_size_curr, 1, device=device)\n",
        "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
        "\n",
        "        ### ----- Train Discriminator p times ----- ###\n",
        "        for _ in range(p):\n",
        "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
        "            fake_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
        "            with torch.no_grad():\n",
        "                gen_imgs = generator(z, fake_labels)\n",
        "\n",
        "            # Real\n",
        "            real_validity = discriminator(real_imgs, real_labels)\n",
        "            d_real_loss = criterion(real_validity, real)\n",
        "\n",
        "            # Fake\n",
        "            fake_validity = discriminator(gen_imgs.detach(), fake_labels)\n",
        "            d_fake_loss = criterion(fake_validity, fake)\n",
        "\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "        ### ----- Train Generator k times ----- ###\n",
        "        for _ in range(k):\n",
        "            # Use batch_size_curr for generator input as well\n",
        "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
        "            gen_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
        "            gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "            validity = discriminator(gen_imgs, gen_labels)\n",
        "            g_loss = criterion(validity, real)  # fool D -> label as real\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}], Step or Batch [{i}/{len(train_loader)}], \"\n",
        "                    f\"D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n",
        "    # Save sample images\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        # Use a fixed size (e.g., 10) for generating samples for visualization,\n",
        "        # or match the batch size if needed for consistency\n",
        "        z = torch.randn(10, z_dim, device=device)\n",
        "        labels = torch.arange(0, 10, dtype=torch.long, device=device)\n",
        "        samples = generator(z, labels)\n",
        "        samples = samples * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "        save_image(samples, f\"c_gan_generated/epoch_{epoch}.png\", nrow=10)\n",
        "    generator.train()"
      ],
      "metadata": {
        "id": "8s-RNKRTkAMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bef7da-c706-4ad6-f1ee-599b950a1497"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step or Batch [0/469], D_loss: 1.3591 | G_loss: 0.6147\n",
            "Epoch [1/50], Step or Batch [200/469], D_loss: 1.3742 | G_loss: 0.6974\n",
            "Epoch [1/50], Step or Batch [400/469], D_loss: 1.3698 | G_loss: 0.6760\n",
            "Epoch [2/50], Step or Batch [0/469], D_loss: 1.3789 | G_loss: 0.6628\n",
            "Epoch [2/50], Step or Batch [200/469], D_loss: 1.4065 | G_loss: 0.6738\n",
            "Epoch [2/50], Step or Batch [400/469], D_loss: 1.4194 | G_loss: 0.6756\n",
            "Epoch [3/50], Step or Batch [0/469], D_loss: 1.4000 | G_loss: 0.6753\n",
            "Epoch [3/50], Step or Batch [200/469], D_loss: 1.3900 | G_loss: 0.7060\n",
            "Epoch [3/50], Step or Batch [400/469], D_loss: 1.3601 | G_loss: 0.6967\n",
            "Epoch [4/50], Step or Batch [0/469], D_loss: 1.3801 | G_loss: 0.6921\n",
            "Epoch [4/50], Step or Batch [200/469], D_loss: 1.3689 | G_loss: 0.7365\n",
            "Epoch [4/50], Step or Batch [400/469], D_loss: 1.3525 | G_loss: 0.7117\n",
            "Epoch [5/50], Step or Batch [0/469], D_loss: 1.3951 | G_loss: 0.6999\n",
            "Epoch [5/50], Step or Batch [200/469], D_loss: 1.3725 | G_loss: 0.7066\n",
            "Epoch [5/50], Step or Batch [400/469], D_loss: 1.4061 | G_loss: 0.6955\n",
            "Epoch [6/50], Step or Batch [0/469], D_loss: 1.3522 | G_loss: 0.7150\n",
            "Epoch [6/50], Step or Batch [200/469], D_loss: 1.3944 | G_loss: 0.6846\n",
            "Epoch [6/50], Step or Batch [400/469], D_loss: 1.4197 | G_loss: 0.7102\n",
            "Epoch [7/50], Step or Batch [0/469], D_loss: 1.3858 | G_loss: 0.6921\n",
            "Epoch [7/50], Step or Batch [200/469], D_loss: 1.4008 | G_loss: 0.6765\n",
            "Epoch [7/50], Step or Batch [400/469], D_loss: 1.3654 | G_loss: 0.7275\n",
            "Epoch [8/50], Step or Batch [0/469], D_loss: 1.3437 | G_loss: 0.7151\n",
            "Epoch [8/50], Step or Batch [200/469], D_loss: 1.3520 | G_loss: 0.7527\n",
            "Epoch [8/50], Step or Batch [400/469], D_loss: 1.3836 | G_loss: 0.7141\n",
            "Epoch [9/50], Step or Batch [0/469], D_loss: 1.3442 | G_loss: 0.7166\n",
            "Epoch [9/50], Step or Batch [200/469], D_loss: 1.3664 | G_loss: 0.7352\n",
            "Epoch [9/50], Step or Batch [400/469], D_loss: 1.3723 | G_loss: 0.6853\n",
            "Epoch [10/50], Step or Batch [0/469], D_loss: 1.3438 | G_loss: 0.6789\n",
            "Epoch [10/50], Step or Batch [200/469], D_loss: 1.3710 | G_loss: 0.6792\n",
            "Epoch [10/50], Step or Batch [400/469], D_loss: 1.4077 | G_loss: 0.7583\n",
            "Epoch [11/50], Step or Batch [0/469], D_loss: 1.4085 | G_loss: 0.7195\n",
            "Epoch [11/50], Step or Batch [200/469], D_loss: 1.2952 | G_loss: 0.7279\n",
            "Epoch [11/50], Step or Batch [400/469], D_loss: 1.4460 | G_loss: 0.7064\n",
            "Epoch [12/50], Step or Batch [0/469], D_loss: 1.3305 | G_loss: 0.9058\n",
            "Epoch [12/50], Step or Batch [200/469], D_loss: 1.4169 | G_loss: 0.6487\n",
            "Epoch [12/50], Step or Batch [400/469], D_loss: 1.3776 | G_loss: 0.5883\n",
            "Epoch [13/50], Step or Batch [0/469], D_loss: 1.3807 | G_loss: 0.7050\n",
            "Epoch [13/50], Step or Batch [200/469], D_loss: 1.4068 | G_loss: 0.7695\n",
            "Epoch [13/50], Step or Batch [400/469], D_loss: 1.3150 | G_loss: 0.7266\n",
            "Epoch [14/50], Step or Batch [0/469], D_loss: 1.3320 | G_loss: 0.8092\n",
            "Epoch [14/50], Step or Batch [200/469], D_loss: 1.4255 | G_loss: 0.6485\n",
            "Epoch [14/50], Step or Batch [400/469], D_loss: 1.3774 | G_loss: 0.7999\n",
            "Epoch [15/50], Step or Batch [0/469], D_loss: 1.3688 | G_loss: 0.7663\n",
            "Epoch [15/50], Step or Batch [200/469], D_loss: 1.3790 | G_loss: 0.7521\n",
            "Epoch [15/50], Step or Batch [400/469], D_loss: 1.3343 | G_loss: 0.7609\n",
            "Epoch [16/50], Step or Batch [0/469], D_loss: 1.3198 | G_loss: 0.7876\n",
            "Epoch [16/50], Step or Batch [200/469], D_loss: 1.3319 | G_loss: 0.7923\n",
            "Epoch [16/50], Step or Batch [400/469], D_loss: 1.3993 | G_loss: 0.7566\n",
            "Epoch [17/50], Step or Batch [0/469], D_loss: 1.3773 | G_loss: 0.8936\n",
            "Epoch [17/50], Step or Batch [200/469], D_loss: 1.3840 | G_loss: 0.7852\n",
            "Epoch [17/50], Step or Batch [400/469], D_loss: 1.2820 | G_loss: 0.6444\n",
            "Epoch [18/50], Step or Batch [0/469], D_loss: 1.3810 | G_loss: 1.5109\n",
            "Epoch [18/50], Step or Batch [200/469], D_loss: 1.3079 | G_loss: 0.6602\n",
            "Epoch [18/50], Step or Batch [400/469], D_loss: 1.4156 | G_loss: 0.7443\n",
            "Epoch [19/50], Step or Batch [0/469], D_loss: 1.3336 | G_loss: 0.7271\n",
            "Epoch [19/50], Step or Batch [200/469], D_loss: 1.3246 | G_loss: 0.8780\n",
            "Epoch [19/50], Step or Batch [400/469], D_loss: 1.3275 | G_loss: 0.8053\n",
            "Epoch [20/50], Step or Batch [0/469], D_loss: 1.3000 | G_loss: 0.8312\n",
            "Epoch [20/50], Step or Batch [200/469], D_loss: 1.3235 | G_loss: 0.7568\n",
            "Epoch [20/50], Step or Batch [400/469], D_loss: 1.4475 | G_loss: 0.5919\n",
            "Epoch [21/50], Step or Batch [0/469], D_loss: 1.3361 | G_loss: 0.7559\n",
            "Epoch [21/50], Step or Batch [200/469], D_loss: 1.3913 | G_loss: 0.7176\n",
            "Epoch [21/50], Step or Batch [400/469], D_loss: 1.3520 | G_loss: 0.7154\n",
            "Epoch [22/50], Step or Batch [0/469], D_loss: 1.4074 | G_loss: 0.7543\n",
            "Epoch [22/50], Step or Batch [200/469], D_loss: 1.3587 | G_loss: 0.7733\n",
            "Epoch [22/50], Step or Batch [400/469], D_loss: 1.3256 | G_loss: 0.7797\n",
            "Epoch [23/50], Step or Batch [0/469], D_loss: 1.3700 | G_loss: 0.7808\n",
            "Epoch [23/50], Step or Batch [200/469], D_loss: 1.3163 | G_loss: 0.8270\n",
            "Epoch [23/50], Step or Batch [400/469], D_loss: 1.2938 | G_loss: 0.9638\n",
            "Epoch [24/50], Step or Batch [0/469], D_loss: 1.3217 | G_loss: 0.6859\n",
            "Epoch [24/50], Step or Batch [200/469], D_loss: 1.2463 | G_loss: 0.6768\n",
            "Epoch [24/50], Step or Batch [400/469], D_loss: 1.3332 | G_loss: 0.9283\n",
            "Epoch [25/50], Step or Batch [0/469], D_loss: 1.3527 | G_loss: 0.7165\n",
            "Epoch [25/50], Step or Batch [200/469], D_loss: 1.3918 | G_loss: 0.9277\n",
            "Epoch [25/50], Step or Batch [400/469], D_loss: 1.3394 | G_loss: 0.6812\n",
            "Epoch [26/50], Step or Batch [0/469], D_loss: 1.4090 | G_loss: 0.6888\n",
            "Epoch [26/50], Step or Batch [200/469], D_loss: 1.3416 | G_loss: 0.7531\n",
            "Epoch [26/50], Step or Batch [400/469], D_loss: 1.3002 | G_loss: 0.7194\n",
            "Epoch [27/50], Step or Batch [0/469], D_loss: 1.3894 | G_loss: 0.8988\n",
            "Epoch [27/50], Step or Batch [200/469], D_loss: 1.2892 | G_loss: 0.7464\n",
            "Epoch [27/50], Step or Batch [400/469], D_loss: 1.3285 | G_loss: 0.8532\n",
            "Epoch [28/50], Step or Batch [0/469], D_loss: 1.2517 | G_loss: 0.9127\n",
            "Epoch [28/50], Step or Batch [200/469], D_loss: 1.2734 | G_loss: 0.9196\n",
            "Epoch [28/50], Step or Batch [400/469], D_loss: 1.3411 | G_loss: 0.7977\n",
            "Epoch [29/50], Step or Batch [0/469], D_loss: 1.2532 | G_loss: 0.8916\n",
            "Epoch [29/50], Step or Batch [200/469], D_loss: 1.4540 | G_loss: 0.6878\n",
            "Epoch [29/50], Step or Batch [400/469], D_loss: 1.3139 | G_loss: 0.7739\n",
            "Epoch [30/50], Step or Batch [0/469], D_loss: 1.1173 | G_loss: 1.2025\n",
            "Epoch [30/50], Step or Batch [200/469], D_loss: 1.3282 | G_loss: 0.5777\n",
            "Epoch [30/50], Step or Batch [400/469], D_loss: 1.3413 | G_loss: 0.7449\n",
            "Epoch [31/50], Step or Batch [0/469], D_loss: 1.1686 | G_loss: 1.0574\n",
            "Epoch [31/50], Step or Batch [200/469], D_loss: 1.2932 | G_loss: 1.0251\n",
            "Epoch [31/50], Step or Batch [400/469], D_loss: 1.2115 | G_loss: 0.8163\n",
            "Epoch [32/50], Step or Batch [0/469], D_loss: 1.1705 | G_loss: 0.9196\n",
            "Epoch [32/50], Step or Batch [200/469], D_loss: 1.2922 | G_loss: 0.8464\n",
            "Epoch [32/50], Step or Batch [400/469], D_loss: 1.2512 | G_loss: 1.0454\n",
            "Epoch [33/50], Step or Batch [0/469], D_loss: 1.2149 | G_loss: 0.9608\n",
            "Epoch [33/50], Step or Batch [200/469], D_loss: 1.2833 | G_loss: 0.9615\n",
            "Epoch [33/50], Step or Batch [400/469], D_loss: 1.2394 | G_loss: 0.8691\n",
            "Epoch [34/50], Step or Batch [0/469], D_loss: 1.3325 | G_loss: 0.9433\n",
            "Epoch [34/50], Step or Batch [200/469], D_loss: 1.3458 | G_loss: 1.0025\n",
            "Epoch [34/50], Step or Batch [400/469], D_loss: 1.2558 | G_loss: 0.8002\n",
            "Epoch [35/50], Step or Batch [0/469], D_loss: 1.2389 | G_loss: 0.9844\n",
            "Epoch [35/50], Step or Batch [200/469], D_loss: 1.2526 | G_loss: 1.0945\n",
            "Epoch [35/50], Step or Batch [400/469], D_loss: 1.3413 | G_loss: 0.9021\n",
            "Epoch [36/50], Step or Batch [0/469], D_loss: 1.3447 | G_loss: 1.0846\n",
            "Epoch [36/50], Step or Batch [200/469], D_loss: 1.2613 | G_loss: 1.0595\n",
            "Epoch [36/50], Step or Batch [400/469], D_loss: 1.2613 | G_loss: 0.9213\n",
            "Epoch [37/50], Step or Batch [0/469], D_loss: 1.2362 | G_loss: 0.8852\n",
            "Epoch [37/50], Step or Batch [200/469], D_loss: 1.2776 | G_loss: 1.0390\n",
            "Epoch [37/50], Step or Batch [400/469], D_loss: 1.3258 | G_loss: 1.1708\n",
            "Epoch [38/50], Step or Batch [0/469], D_loss: 1.3963 | G_loss: 0.7911\n",
            "Epoch [38/50], Step or Batch [200/469], D_loss: 1.3291 | G_loss: 0.9670\n",
            "Epoch [38/50], Step or Batch [400/469], D_loss: 1.2777 | G_loss: 0.7168\n",
            "Epoch [39/50], Step or Batch [0/469], D_loss: 1.3295 | G_loss: 0.8854\n",
            "Epoch [39/50], Step or Batch [200/469], D_loss: 1.2791 | G_loss: 0.8568\n",
            "Epoch [39/50], Step or Batch [400/469], D_loss: 1.2183 | G_loss: 0.8966\n",
            "Epoch [40/50], Step or Batch [0/469], D_loss: 1.2059 | G_loss: 1.1699\n",
            "Epoch [40/50], Step or Batch [200/469], D_loss: 1.2209 | G_loss: 0.9252\n",
            "Epoch [40/50], Step or Batch [400/469], D_loss: 1.2157 | G_loss: 0.8582\n",
            "Epoch [41/50], Step or Batch [0/469], D_loss: 1.1530 | G_loss: 0.8853\n",
            "Epoch [41/50], Step or Batch [200/469], D_loss: 1.3065 | G_loss: 1.0418\n",
            "Epoch [41/50], Step or Batch [400/469], D_loss: 1.2075 | G_loss: 1.2016\n",
            "Epoch [42/50], Step or Batch [0/469], D_loss: 1.2379 | G_loss: 0.9894\n",
            "Epoch [42/50], Step or Batch [200/469], D_loss: 1.2431 | G_loss: 1.0188\n",
            "Epoch [42/50], Step or Batch [400/469], D_loss: 1.2012 | G_loss: 0.9542\n",
            "Epoch [43/50], Step or Batch [0/469], D_loss: 1.3129 | G_loss: 1.0267\n",
            "Epoch [43/50], Step or Batch [200/469], D_loss: 1.2140 | G_loss: 1.1301\n",
            "Epoch [43/50], Step or Batch [400/469], D_loss: 1.2833 | G_loss: 0.9811\n",
            "Epoch [44/50], Step or Batch [0/469], D_loss: 1.2164 | G_loss: 1.0143\n",
            "Epoch [44/50], Step or Batch [200/469], D_loss: 1.2773 | G_loss: 1.0022\n",
            "Epoch [44/50], Step or Batch [400/469], D_loss: 1.1657 | G_loss: 1.1244\n",
            "Epoch [45/50], Step or Batch [0/469], D_loss: 1.3425 | G_loss: 0.9209\n",
            "Epoch [45/50], Step or Batch [200/469], D_loss: 1.2782 | G_loss: 1.0969\n",
            "Epoch [45/50], Step or Batch [400/469], D_loss: 1.2542 | G_loss: 1.0930\n",
            "Epoch [46/50], Step or Batch [0/469], D_loss: 1.3023 | G_loss: 0.7901\n",
            "Epoch [46/50], Step or Batch [200/469], D_loss: 1.2220 | G_loss: 0.9797\n",
            "Epoch [46/50], Step or Batch [400/469], D_loss: 1.3258 | G_loss: 1.0833\n",
            "Epoch [47/50], Step or Batch [0/469], D_loss: 1.2074 | G_loss: 1.1074\n",
            "Epoch [47/50], Step or Batch [200/469], D_loss: 1.2217 | G_loss: 1.2567\n",
            "Epoch [47/50], Step or Batch [400/469], D_loss: 1.1694 | G_loss: 1.3414\n",
            "Epoch [48/50], Step or Batch [0/469], D_loss: 1.3235 | G_loss: 1.1905\n",
            "Epoch [48/50], Step or Batch [200/469], D_loss: 1.1982 | G_loss: 1.3440\n",
            "Epoch [48/50], Step or Batch [400/469], D_loss: 1.2208 | G_loss: 1.4834\n",
            "Epoch [49/50], Step or Batch [0/469], D_loss: 1.2143 | G_loss: 0.8999\n",
            "Epoch [49/50], Step or Batch [200/469], D_loss: 1.3690 | G_loss: 0.9958\n",
            "Epoch [49/50], Step or Batch [400/469], D_loss: 1.2735 | G_loss: 0.9367\n",
            "Epoch [50/50], Step or Batch [0/469], D_loss: 1.2012 | G_loss: 0.9161\n",
            "Epoch [50/50], Step or Batch [200/469], D_loss: 1.2360 | G_loss: 1.0702\n",
            "Epoch [50/50], Step or Batch [400/469], D_loss: 1.2432 | G_loss: 0.9750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_digit_images(generator, digit, num_samples=16, save_path=None):\n",
        "    generator.eval()\n",
        "    # with torch.no_grad():\n",
        "    z = torch.randn(num_samples, z_dim, device=device).to(device)\n",
        "    labels = torch.full((num_samples,), digit, device=device, dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(z, labels)\n",
        "        gen_imgs = gen_imgs * 0.5 + 0.5\n",
        "    if save_path:\n",
        "        save_image(gen_imgs, save_path, nrow=4)\n",
        "        print(f\"Generated images saved to {save_path}\")\n",
        "    return gen_imgs"
      ],
      "metadata": {
        "id": "eOf5RII5phI0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_digit_images(generator, digit=5, num_samples=16, save_path=\"cgan_generated_imgs/digit_5.png\")"
      ],
      "metadata": {
        "id": "Ir3AbgqjnVwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "8fab83ba-8cbd-40b7-8c73-e0dafaece593"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cgan_generated_imgs/digit_5.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2956338158.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_digit_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cgan_generated_imgs/digit_5.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2494952786.py\u001b[0m in \u001b[0;36mgenerate_digit_images\u001b[0;34m(generator, digit, num_samples, save_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_imgs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated images saved to {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_imgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cgan_generated_imgs/digit_5.png'"
          ]
        }
      ]
    }
  ]
}
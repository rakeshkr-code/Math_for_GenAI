{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p8kjVKoDthhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "# import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "z_dim = 100\n",
        "image_size = 28\n",
        "num_classes = 10\n",
        "channels = 1\n",
        "epochs = 40\n",
        "lr = 0.0002\n",
        "beta1 = 0.5 # Adam optimizer beta1\n",
        "# beta2 = 0.999\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide : {device}\")\n",
        "\n",
        "# Create output folder\n",
        "# if not os.path.exists(\"generated_imgs\"):\n",
        "os.makedirs(\"c_gan_generated\", exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAXnM5ybXS5M",
        "outputId": "ad8239d9-f922-4ab7-bec0-f360546bc7ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform: Normalize images between [-1, 1] (because Tanh will be used as output)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))    # Normalize between [-1, 1]\n",
        "])\n",
        "\n",
        "# Load MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "oNsOrVYewFgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab760b9f-a278-4d19-b935-cb63cf6a0841"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 56.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.61MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.9MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.03MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceda4a32"
      },
      "source": [
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, num_classes, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.img_shape = img_shape\n",
        "        input_dim = z_dim + num_classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (N, z_dim, 1, 1)\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, noise, labels):\n",
        "        # Concatenate noise and label embedding\n",
        "        x = torch.cat([noise, self.label_emb(labels)], dim=1)\n",
        "        img = self.model(x)\n",
        "        img = img.view(x.size(0), *self.img_shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "xcAAtxsgzZ0g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        input_dim = int(torch.prod(torch.tensor(img_shape))) + num_classes\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (N, 1, 28, 28)\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, img, labels):\n",
        "        # Flatten image and concatenate label\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        x = torch.cat([img_flat, self.label_emb(labels)], dim=1)\n",
        "        validity = self.model(x)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "rN6u3tlq6Y23"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = (channels, image_size, image_size)\n",
        "# Models\n",
        "generator = Generator(z_dim, num_classes, img_shape).to(device)\n",
        "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Loss - Binary Cross Entropy Loss\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "kxCRtq_HCxeT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(epoch):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, z_dim, 1, 1).to(device)\n",
        "        fake_images = generator(z)\n",
        "        fake_images = fake_images * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "        save_image(fake_images, f\"generated_imgs/sample_epoch_{epoch}.png\", nrow=8)\n",
        "    generator.train()"
      ],
      "metadata": {
        "id": "fUwFbxPs9o79"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3   # Generator updates per iteration\n",
        "p = 1   # Discriminator updates per iterations"
      ],
      "metadata": {
        "id": "FCtxK7Bui44H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, epochs+1):\n",
        "    for i, (real_imgs, real_labels) in enumerate(train_loader):\n",
        "        batch_size_curr = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        real_labels = real_labels.to(device)\n",
        "\n",
        "        # Use the actual batch size for creating target tensors\n",
        "        real = torch.ones(batch_size_curr, 1, device=device)\n",
        "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
        "\n",
        "        ### ----- Train Discriminator p times ----- ###\n",
        "        for _ in range(p):\n",
        "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
        "            fake_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
        "            with torch.no_grad():\n",
        "                gen_imgs = generator(z, fake_labels)\n",
        "\n",
        "            # Real\n",
        "            real_validity = discriminator(real_imgs, real_labels)\n",
        "            d_real_loss = criterion(real_validity, real)\n",
        "\n",
        "            # Fake\n",
        "            fake_validity = discriminator(gen_imgs.detach(), fake_labels)\n",
        "            d_fake_loss = criterion(fake_validity, fake)\n",
        "\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "        ### ----- Train Generator k times ----- ###\n",
        "        for _ in range(k):\n",
        "            # Use batch_size_curr for generator input as well\n",
        "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
        "            gen_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
        "            gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "            validity = discriminator(gen_imgs, gen_labels)\n",
        "            g_loss = criterion(validity, real)  # fool D -> label as real\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}], Step or Batch [{i}/{len(train_loader)}], \"\n",
        "                    f\"D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n",
        "    # Save sample images\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        # Use a fixed size (e.g., 10) for generating samples for visualization,\n",
        "        # or match the batch size if needed for consistency\n",
        "        z = torch.randn(10, z_dim, device=device)\n",
        "        labels = torch.arange(0, 10, dtype=torch.long, device=device)\n",
        "        samples = generator(z, labels)\n",
        "        samples = samples * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "        save_image(samples, f\"c_gan_generated/epoch_{epoch}.png\", nrow=10)\n",
        "    generator.train()"
      ],
      "metadata": {
        "id": "8s-RNKRTkAMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25fbb85-caf4-49b3-a866-e2659f2913a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40], Step or Batch [0/469], D_loss: 1.4281 | G_loss: 0.5850\n",
            "Epoch [1/40], Step or Batch [200/469], D_loss: 1.3905 | G_loss: 0.6696\n",
            "Epoch [1/40], Step or Batch [400/469], D_loss: 1.4019 | G_loss: 0.6522\n",
            "Epoch [2/40], Step or Batch [0/469], D_loss: 1.3872 | G_loss: 0.6861\n",
            "Epoch [2/40], Step or Batch [200/469], D_loss: 1.4066 | G_loss: 0.6998\n",
            "Epoch [2/40], Step or Batch [400/469], D_loss: 1.3729 | G_loss: 0.6756\n",
            "Epoch [3/40], Step or Batch [0/469], D_loss: 1.3848 | G_loss: 0.6831\n",
            "Epoch [3/40], Step or Batch [200/469], D_loss: 1.3882 | G_loss: 0.6731\n",
            "Epoch [3/40], Step or Batch [400/469], D_loss: 1.3871 | G_loss: 0.7127\n",
            "Epoch [4/40], Step or Batch [0/469], D_loss: 1.3836 | G_loss: 0.7271\n",
            "Epoch [4/40], Step or Batch [200/469], D_loss: 1.3610 | G_loss: 0.7034\n",
            "Epoch [4/40], Step or Batch [400/469], D_loss: 1.3480 | G_loss: 0.7238\n",
            "Epoch [5/40], Step or Batch [0/469], D_loss: 1.3800 | G_loss: 0.7209\n",
            "Epoch [5/40], Step or Batch [200/469], D_loss: 1.3835 | G_loss: 0.7106\n",
            "Epoch [5/40], Step or Batch [400/469], D_loss: 1.3807 | G_loss: 0.7601\n",
            "Epoch [6/40], Step or Batch [0/469], D_loss: 1.3883 | G_loss: 0.7199\n",
            "Epoch [6/40], Step or Batch [200/469], D_loss: 1.3691 | G_loss: 0.7412\n",
            "Epoch [6/40], Step or Batch [400/469], D_loss: 1.3802 | G_loss: 0.7006\n",
            "Epoch [7/40], Step or Batch [0/469], D_loss: 1.4281 | G_loss: 0.6758\n",
            "Epoch [7/40], Step or Batch [200/469], D_loss: 1.4266 | G_loss: 0.7010\n",
            "Epoch [7/40], Step or Batch [400/469], D_loss: 1.3264 | G_loss: 0.7094\n",
            "Epoch [8/40], Step or Batch [0/469], D_loss: 1.3977 | G_loss: 0.6814\n",
            "Epoch [8/40], Step or Batch [200/469], D_loss: 1.3712 | G_loss: 0.7597\n",
            "Epoch [8/40], Step or Batch [400/469], D_loss: 1.3574 | G_loss: 0.7058\n",
            "Epoch [9/40], Step or Batch [0/469], D_loss: 1.4338 | G_loss: 0.6735\n",
            "Epoch [9/40], Step or Batch [200/469], D_loss: 1.3547 | G_loss: 0.7696\n",
            "Epoch [9/40], Step or Batch [400/469], D_loss: 1.3690 | G_loss: 0.6960\n",
            "Epoch [10/40], Step or Batch [0/469], D_loss: 1.4248 | G_loss: 0.6825\n",
            "Epoch [10/40], Step or Batch [200/469], D_loss: 1.3743 | G_loss: 0.7205\n",
            "Epoch [10/40], Step or Batch [400/469], D_loss: 1.3642 | G_loss: 0.7199\n",
            "Epoch [11/40], Step or Batch [0/469], D_loss: 1.3651 | G_loss: 0.7265\n",
            "Epoch [11/40], Step or Batch [200/469], D_loss: 1.3867 | G_loss: 0.7138\n",
            "Epoch [11/40], Step or Batch [400/469], D_loss: 1.4169 | G_loss: 0.6837\n",
            "Epoch [12/40], Step or Batch [0/469], D_loss: 1.4076 | G_loss: 0.6614\n",
            "Epoch [12/40], Step or Batch [200/469], D_loss: 1.3943 | G_loss: 0.6916\n",
            "Epoch [12/40], Step or Batch [400/469], D_loss: 1.3474 | G_loss: 0.7307\n",
            "Epoch [13/40], Step or Batch [0/469], D_loss: 1.3737 | G_loss: 0.7517\n",
            "Epoch [13/40], Step or Batch [200/469], D_loss: 1.3567 | G_loss: 0.7120\n",
            "Epoch [13/40], Step or Batch [400/469], D_loss: 1.3633 | G_loss: 0.7705\n",
            "Epoch [14/40], Step or Batch [0/469], D_loss: 1.3724 | G_loss: 0.7219\n",
            "Epoch [14/40], Step or Batch [200/469], D_loss: 1.3777 | G_loss: 0.7095\n",
            "Epoch [14/40], Step or Batch [400/469], D_loss: 1.3938 | G_loss: 0.7821\n",
            "Epoch [15/40], Step or Batch [0/469], D_loss: 1.3857 | G_loss: 0.6869\n",
            "Epoch [15/40], Step or Batch [200/469], D_loss: 1.3854 | G_loss: 0.6689\n",
            "Epoch [15/40], Step or Batch [400/469], D_loss: 1.3967 | G_loss: 0.6296\n",
            "Epoch [16/40], Step or Batch [0/469], D_loss: 1.3448 | G_loss: 0.7610\n",
            "Epoch [16/40], Step or Batch [200/469], D_loss: 1.3185 | G_loss: 0.7209\n",
            "Epoch [16/40], Step or Batch [400/469], D_loss: 1.3889 | G_loss: 0.7858\n",
            "Epoch [17/40], Step or Batch [0/469], D_loss: 1.3795 | G_loss: 0.6771\n",
            "Epoch [17/40], Step or Batch [200/469], D_loss: 1.3266 | G_loss: 0.7500\n",
            "Epoch [17/40], Step or Batch [400/469], D_loss: 1.3723 | G_loss: 0.7388\n",
            "Epoch [18/40], Step or Batch [0/469], D_loss: 1.3944 | G_loss: 0.8380\n",
            "Epoch [18/40], Step or Batch [200/469], D_loss: 1.3783 | G_loss: 0.7911\n",
            "Epoch [18/40], Step or Batch [400/469], D_loss: 1.3331 | G_loss: 0.6786\n",
            "Epoch [19/40], Step or Batch [0/469], D_loss: 1.3486 | G_loss: 0.6365\n",
            "Epoch [19/40], Step or Batch [200/469], D_loss: 1.3145 | G_loss: 0.8100\n",
            "Epoch [19/40], Step or Batch [400/469], D_loss: 1.3500 | G_loss: 0.7473\n",
            "Epoch [20/40], Step or Batch [0/469], D_loss: 1.3231 | G_loss: 0.6734\n",
            "Epoch [20/40], Step or Batch [200/469], D_loss: 1.3512 | G_loss: 0.7648\n",
            "Epoch [20/40], Step or Batch [400/469], D_loss: 1.3067 | G_loss: 0.8377\n",
            "Epoch [21/40], Step or Batch [0/469], D_loss: 1.2745 | G_loss: 0.7395\n",
            "Epoch [21/40], Step or Batch [200/469], D_loss: 1.4288 | G_loss: 0.7423\n",
            "Epoch [21/40], Step or Batch [400/469], D_loss: 1.2845 | G_loss: 0.8238\n",
            "Epoch [22/40], Step or Batch [0/469], D_loss: 1.3674 | G_loss: 0.7820\n",
            "Epoch [22/40], Step or Batch [200/469], D_loss: 1.3151 | G_loss: 0.9526\n",
            "Epoch [22/40], Step or Batch [400/469], D_loss: 1.5272 | G_loss: 0.6161\n",
            "Epoch [23/40], Step or Batch [0/469], D_loss: 1.3884 | G_loss: 0.6822\n",
            "Epoch [23/40], Step or Batch [200/469], D_loss: 1.2669 | G_loss: 0.8312\n",
            "Epoch [23/40], Step or Batch [400/469], D_loss: 1.2833 | G_loss: 0.9163\n",
            "Epoch [24/40], Step or Batch [0/469], D_loss: 1.3162 | G_loss: 0.8874\n",
            "Epoch [24/40], Step or Batch [200/469], D_loss: 1.2936 | G_loss: 0.7905\n",
            "Epoch [24/40], Step or Batch [400/469], D_loss: 1.2637 | G_loss: 0.9012\n",
            "Epoch [25/40], Step or Batch [0/469], D_loss: 1.2262 | G_loss: 0.7483\n",
            "Epoch [25/40], Step or Batch [200/469], D_loss: 1.2167 | G_loss: 0.8628\n",
            "Epoch [25/40], Step or Batch [400/469], D_loss: 1.2997 | G_loss: 0.8344\n",
            "Epoch [26/40], Step or Batch [0/469], D_loss: 1.2860 | G_loss: 0.9109\n",
            "Epoch [26/40], Step or Batch [200/469], D_loss: 1.2759 | G_loss: 0.8408\n",
            "Epoch [26/40], Step or Batch [400/469], D_loss: 1.2913 | G_loss: 0.8624\n",
            "Epoch [27/40], Step or Batch [0/469], D_loss: 1.2865 | G_loss: 0.9126\n",
            "Epoch [27/40], Step or Batch [200/469], D_loss: 1.2984 | G_loss: 0.7217\n",
            "Epoch [27/40], Step or Batch [400/469], D_loss: 1.2517 | G_loss: 0.8546\n",
            "Epoch [28/40], Step or Batch [0/469], D_loss: 1.2337 | G_loss: 0.7589\n",
            "Epoch [28/40], Step or Batch [200/469], D_loss: 1.2755 | G_loss: 0.9160\n",
            "Epoch [28/40], Step or Batch [400/469], D_loss: 1.3503 | G_loss: 0.8379\n",
            "Epoch [29/40], Step or Batch [0/469], D_loss: 1.2669 | G_loss: 1.0745\n",
            "Epoch [29/40], Step or Batch [200/469], D_loss: 1.3327 | G_loss: 0.7637\n",
            "Epoch [29/40], Step or Batch [400/469], D_loss: 1.2362 | G_loss: 0.9288\n",
            "Epoch [30/40], Step or Batch [0/469], D_loss: 1.2697 | G_loss: 1.1221\n",
            "Epoch [30/40], Step or Batch [200/469], D_loss: 1.2664 | G_loss: 0.9343\n",
            "Epoch [30/40], Step or Batch [400/469], D_loss: 1.2695 | G_loss: 0.9602\n",
            "Epoch [31/40], Step or Batch [0/469], D_loss: 1.1911 | G_loss: 1.1027\n",
            "Epoch [31/40], Step or Batch [200/469], D_loss: 1.1899 | G_loss: 0.9987\n",
            "Epoch [31/40], Step or Batch [400/469], D_loss: 1.2704 | G_loss: 0.9686\n",
            "Epoch [32/40], Step or Batch [0/469], D_loss: 1.4024 | G_loss: 0.8502\n",
            "Epoch [32/40], Step or Batch [200/469], D_loss: 1.2482 | G_loss: 0.8319\n",
            "Epoch [32/40], Step or Batch [400/469], D_loss: 1.2931 | G_loss: 1.1079\n",
            "Epoch [33/40], Step or Batch [0/469], D_loss: 1.1914 | G_loss: 0.8840\n",
            "Epoch [33/40], Step or Batch [200/469], D_loss: 1.2201 | G_loss: 0.9010\n",
            "Epoch [33/40], Step or Batch [400/469], D_loss: 1.2498 | G_loss: 1.0499\n",
            "Epoch [34/40], Step or Batch [0/469], D_loss: 1.2519 | G_loss: 1.0797\n",
            "Epoch [34/40], Step or Batch [200/469], D_loss: 1.2947 | G_loss: 1.1161\n",
            "Epoch [34/40], Step or Batch [400/469], D_loss: 1.2686 | G_loss: 0.8302\n",
            "Epoch [35/40], Step or Batch [0/469], D_loss: 1.2472 | G_loss: 1.0799\n",
            "Epoch [35/40], Step or Batch [200/469], D_loss: 1.2395 | G_loss: 1.2616\n",
            "Epoch [35/40], Step or Batch [400/469], D_loss: 1.2851 | G_loss: 0.8746\n",
            "Epoch [36/40], Step or Batch [0/469], D_loss: 1.2060 | G_loss: 0.8882\n",
            "Epoch [36/40], Step or Batch [200/469], D_loss: 1.2948 | G_loss: 1.2867\n",
            "Epoch [36/40], Step or Batch [400/469], D_loss: 1.2942 | G_loss: 0.8958\n",
            "Epoch [37/40], Step or Batch [0/469], D_loss: 1.2448 | G_loss: 0.9278\n",
            "Epoch [37/40], Step or Batch [200/469], D_loss: 1.2701 | G_loss: 0.9912\n",
            "Epoch [37/40], Step or Batch [400/469], D_loss: 1.2933 | G_loss: 1.0157\n",
            "Epoch [38/40], Step or Batch [0/469], D_loss: 1.2871 | G_loss: 1.1475\n",
            "Epoch [38/40], Step or Batch [200/469], D_loss: 1.3017 | G_loss: 0.8851\n",
            "Epoch [38/40], Step or Batch [400/469], D_loss: 1.1893 | G_loss: 0.9925\n",
            "Epoch [39/40], Step or Batch [0/469], D_loss: 1.2233 | G_loss: 1.0866\n",
            "Epoch [39/40], Step or Batch [200/469], D_loss: 1.1556 | G_loss: 0.9892\n",
            "Epoch [39/40], Step or Batch [400/469], D_loss: 1.2432 | G_loss: 1.0535\n",
            "Epoch [40/40], Step or Batch [0/469], D_loss: 1.2365 | G_loss: 0.8546\n",
            "Epoch [40/40], Step or Batch [200/469], D_loss: 1.3250 | G_loss: 1.0770\n",
            "Epoch [40/40], Step or Batch [400/469], D_loss: 1.2960 | G_loss: 0.9516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_digit_images(generator, digit, num_samples=16, save_path=None):\n",
        "    generator.eval()\n",
        "    # with torch.no_grad():\n",
        "    z = torch.randn(num_samples, z_dim, device=device).to(device)\n",
        "    labels = torch.full((num_samples,), digit, device=device, dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(z, labels)\n",
        "        gen_imgs = gen_imgs * 0.5 + 0.5\n",
        "    if save_path:\n",
        "        save_image(gen_imgs, save_path, nrow=4)\n",
        "        print(f\"Generated images saved to {save_path}\")\n",
        "    return gen_imgs"
      ],
      "metadata": {
        "id": "eOf5RII5phI0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_digit_images(generator, digit=5, num_samples=16, save_path=\"cgan_generated_imgs/digit_5.png\")"
      ],
      "metadata": {
        "id": "Ir3AbgqjnVwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d1bfda-02ab-4d2e-a96f-b1b8fa5ccd3e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated images saved to cgan_generated_imgs/digit_5.png\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}